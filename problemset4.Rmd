---
title: "Problem Set 4"
output: html_document
date: "2024-04-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

A.

```{r}
library('tidyverse')
library('rsample')
library(tree)
library(randomForest)
library(randomForestExplainer)

options(scipen=99)
set.seed(310)

movies <- read.csv("datasets/IMDB_movies.csv")

movies_clean <- movies %>% 
  filter(budget < 4e+08) %>% 
  filter(content_rating != "", content_rating != "Not Rated", plot_keywords != "", !is.na(gross))

movies_clean <- movies_clean %>% 
  mutate(genre_main = unlist(map(strsplit(as.character(movies_clean$genres),"\\|"), 1)), 
         plot_main = unlist(map(strsplit(as.character(movies_clean$plot_keywords), "\\|"), 1)), 
         grossM = gross/1e+06, 
         budgetM = budget/1e+06)

movies_clean <- movies_clean %>% 
  mutate(genre_main = fct_lump(genre_main,7), 
         plot_first = fct_lump(plot_main, 20), 
         content_rating = fct_lump(content_rating,4), 
         country = fct_lump(country, 8), 
         language = fct_lump(language, 4), 
         cast_total_facebook_likes000s = cast_total_facebook_likes/1000) %>% 
  drop_na()

top_director <- movies_clean %>% 
  group_by(director_name) %>% 
  summarize(num_films = n()) %>% 
  top_frac(0.1) %>% 
  mutate(top_director = 1) %>% 
  select(-num_films)

movies_clean <- movies_clean %>% 
  left_join(top_director, by = "director_name") %>% 
  mutate(top_director = replace_na(top_director, 0)) %>% 
  select(-c(director_name, actor_2_name, gross, genres, actor_1_name, movie_title, 
            plot_keywords, budget, color, 
            aspect_ratio, plot_main, actor_2_facebook_likes, 
            color, num_critic_for_reviews, num_voted_users, num_user_for_reviews, 
            actor_2_facebook_likes))


movies_split <- initial_split(movies_clean, prop=0.7)
movies_train <- training(movies_split)
movies_test <- testing(movies_split)

```


```{r}

library(tidyverse)
library(rsample)
library(tree)
library(randomForest)
library(randomForestExplainer)

```

B.

```{r}
library(ggridges)
ggplot(movies_train, aes(grossM, plot_first , fill = plot_first)) +
  geom_density_ridges() +  scale_x_continuous(limits = c(200, 1000)) +
  labs(title = "Distribution of GrossM by Main Plot Keywords") + theme_ridges()

```

# Other with 56, alien with 2, assassin with 1, battle with 1, college with 1, and death with 1.
#this code would not knit because blockbusters object was not found when knitting


blockbuster_keywords <- blockbusters %>%
  group_by(plot_first) %>%
  summarize(count = n(), average_gross = mean(grossM)) %>%
  arrange(desc(count))

print(blockbuster_keywords)

1c/d) Plot the fitted tree using the plot() function. Use text() to add text to the object. You can use digit=2 and pretty=0 options in text function. You can also use cex option in text function to change the font size.




```{r}


tree_model <- tree(grossM ~ ., data = movies_train)
summary(tree_model)
plot(tree_model)
text(tree_model, digits=2, pretty=0, cex=0.4)


```
E.

# The path to get to the leaf node's for blockbuster movies is as follows: The movie must have a budget greater than 107.5M, an imdb score greater than 7.45, and then a budget greater than 181.5. According to the model, if a movie meets all of those criteria, it will be a blockbuster (meaning it grosses more than 300M).

# Movies with the lowest revenue have the following path: the budget is less than 107.5, then if the budget is less than 31.75, and the title year is older than 1992.5, then according to my model the model will have a very low gross which is the recipe for disaster.  





```{r}

library(lattice)
predictions_train <- predict(tree_model, movies_train)
predictions_test <- predict(tree_model, movies_test)


library(caret)

# Calculate RMSE for the training set
rmse_train <- RMSE(predictions_train, movies_train$grossM)

# Calculate RMSE for the testing set
rmse_test <- RMSE(predictions_test, movies_test$grossM)

print(rmse_train)
print(rmse_test)
```


```{r, echo = FALSE}


cv_tree <- cv.tree(tree_model)
plot(cv_tree)

```

I would keep the tree size at 13, no need to prune any of the leafs. The error keeps going down untill we reach our max size of 13.



H. Estimate a random forest with 200 trees. Use mtry = 8 as a parameter. Describe what the mtry parameter does. Also I.  Get predicted values from the random forest model in the training and test set and calculate RMSE for both.

```{r}

rf_model <- randomForest(grossM ~ ., data = movies_train, ntree = 200, mtry = 8, importance = TRUE)
pred_rf_train <- predict(rf_model, movies_train)
pred_rf_test <- predict(rf_model, movies_test)
print(rf_model)
plot(rf_model)

# Calculate RMSE for the training set
rmse1_train <- RMSE(pred_rf_train, movies_train$grossM)

# Calculate RMSE for the testing set
rmse1_test <- RMSE(pred_rf_test, movies_test$grossM)


rmse1_train

rmse1_test

```

# mtry of 8 means that at each split of the tree, the model only considers 8 of the variables/features for the given split.



J. What features are most important.

```{r}
importance(rf_model)
```

# Budget is most important, than imdb_score, than movie_facebook_likes, Country, etc.

K.

```{r}
plot_min_depth_distribution(rf_model)

```

#budget has the lowest depth on average across all 200 trees. Meaning it will most likely be the Root node, or children of the root node.



L. The randomtreeforest model has a test error about 10 million $ lower than the tree model. That being said I would choose the RandomforestTree model. The random forest model performs better than the original tree model because it includes randomness through the process of bagging, or bootstrapping aggregation. This practice generally reduces variance in a model because it samples subsets of the data, runs it through the same model, then averages this result into a final model. Which ends up being ultimately more accurate than running a model on an original training dataset. 

```{r pressure, echo=FALSE}
plot(pressure)
```

